import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import joblib
import warnings
import sys
import subprocess
import json

warnings.filterwarnings('ignore')


# –ü–†–û–í–ï–†–ö–ê –ò –£–°–¢–ê–ù–û–í–ö–ê LightGBM
def install_package(package_name):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–∞ –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"""
    try:
        __import__(package_name)
        print(f"‚úì {package_name} —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return True
    except ImportError:
        print(f"‚è≥ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ {package_name}...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package_name])
            print(f"‚úì {package_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return True
        except subprocess.CalledProcessError:
            print(f"‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å {package_name}")
            return False


# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã
CATBOOST_AVAILABLE = install_package('catboost')
LIGHTGBM_AVAILABLE = install_package('lightgbm')

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
if CATBOOST_AVAILABLE:
    from catboost import CatBoostRegressor

if LIGHTGBM_AVAILABLE:
    import lightgbm as lgb

# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
try:
    df = pd.read_csv('/Users/mariabug/ML/data/combined_ohe.csv')
    print("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {df.shape}")

except FileNotFoundError:
    print("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
    raise FileNotFoundError("–§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏ –Ω–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
    print("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
    raise

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
print("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:")
print(df.info())
print(f"\n–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
print(df.isnull().sum().sum())

# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
df = df.dropna(subset=['price_log'])

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
X = df.drop('price_log', axis=1)
y = df['price_log']

print(f"\n–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (price_log) —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
print(f"Min: {y.min():.2f}, Max: {y.max():.2f}, Mean: {y.mean():.2f}")

# –£–°–ö–û–†–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–ü–£–°–ö–û–í
print(f"\n–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {X.isnull().sum().sum()}")

# –ë—ã—Å—Ç—Ä–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
numeric_columns = X.select_dtypes(include=[np.number]).columns
X[numeric_columns] = X[numeric_columns].fillna(X[numeric_columns].median())

# –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö - –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–ª–Ω—è–µ–º 'unknown'
categorical_columns = X.select_dtypes(include=['object']).columns
for col in categorical_columns:
    X[col] = X[col].fillna('unknown')

print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {X.isnull().sum().sum()}")

# –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í –í –í–´–ë–û–†–ö–ê–•
print("\n" + "=" * 80)
print("–ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í –í –í–´–ë–û–†–ö–ê–•")
print("=" * 80)


def check_and_remove_duplicates(X_train, X_test, y_train, y_test):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –º–µ–∂–¥—É train –∏ test –≤—ã–±–æ—Ä–∫–∞–º–∏"""
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ train
    train_duplicates = X_train.duplicated().sum()
    print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ train –≤—ã–±–æ—Ä–∫–µ: {train_duplicates}")

    if train_duplicates > 0:
        X_train = X_train.drop_duplicates()
        y_train = y_train[X_train.index]
        print(f"–£–¥–∞–ª–µ–Ω–æ {train_duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ train")

    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ test
    test_duplicates = X_test.duplicated().sum()
    print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ test –≤—ã–±–æ—Ä–∫–µ: {test_duplicates}")

    if test_duplicates > 0:
        X_test = X_test.drop_duplicates()
        y_test = y_test[X_test.index]
        print(f"–£–¥–∞–ª–µ–Ω–æ {test_duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ test")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –º–µ–∂–¥—É train –∏ test
    combined = pd.concat([X_train, X_test])
    between_duplicates = combined.duplicated().sum()
    print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –º–µ–∂–¥—É train –∏ test: {between_duplicates}")

    if between_duplicates > 0:
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ test, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ train
        train_hashes = pd.util.hash_pandas_object(X_train).values
        test_hashes = pd.util.hash_pandas_object(X_test).values

        train_hash_set = set(train_hashes)
        duplicate_mask = np.array([h in train_hash_set for h in test_hashes])

        X_test = X_test[~duplicate_mask]
        y_test = y_test[~duplicate_mask]
        print(f"–£–¥–∞–ª–µ–Ω–æ {duplicate_mask.sum()} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ test (–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ train)")

    print(f"–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã: Train {X_train.shape}, Test {X_test.shape}")
    return X_train, X_test, y_train, y_test


# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•
print(f"\n–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º: {X.shape}")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º —Ü–µ–Ω—ã
y_quantiles = pd.qcut(y, q=4, labels=False)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y_quantiles
)

print(f"–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
print(f"–û–±—É—á–∞—é—â–∞—è: {X_train.shape}")
print(f"–¢–µ—Å—Ç–æ–≤–∞—è: {X_test.shape}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
X_train, X_test, y_train, y_test = check_and_remove_duplicates(X_train, X_test, y_train, y_test)


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def get_categorical_features(X):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    categorical_features = X.select_dtypes(include=['object']).columns.tolist()
    return categorical_features


# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –†–ê–°–ß–ï–¢–ê R¬≤ –ü–û –¶–ï–ù–û–í–´–ú –î–ò–ê–ü–ê–ó–û–ù–ê–ú
def calculate_price_range_r2(y_true, y_pred, price_ranges=None):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç R¬≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
    """
    if price_ranges is None:
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º
        price_ranges = [0, 500000, 1000000, 2000000, 5000000, np.inf]

    results = {}
    total_samples = len(y_true)
    weighted_r2 = 0

    for i in range(len(price_ranges) - 1):
        low = price_ranges[i]
        high = price_ranges[i + 1]

        # –ú–∞—Å–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        mask = (y_true >= low) & (y_true < high)

        if np.sum(mask) > 5:  # –ú–∏–Ω–∏–º—É–º 5 samples –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ R¬≤
            y_true_range = y_true[mask]
            y_pred_range = y_pred[mask]

            if len(np.unique(y_true_range)) > 1:  # –ù—É–∂–Ω–∞ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è R¬≤
                r2 = r2_score(y_true_range, y_pred_range)
            else:
                r2 = np.nan

            n_samples = len(y_true_range)
            weight = n_samples / total_samples

            results[f'{low:,} - {high:,}'] = {
                'r2': r2,
                'samples': n_samples,
                'weight': weight
            }

            if not np.isnan(r2):
                weighted_r2 += r2 * weight
        else:
            results[f'{low:,} - {high:,}'] = {
                'r2': np.nan,
                'samples': np.sum(mask),
                'weight': np.sum(mask) / total_samples
            }

    return {
        'range_r2': results,
        'weighted_r2': weighted_r2
    }


# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø
def analyze_overfitting(results):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "=" * 80)
    print("–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 80)

    overfitting_analysis = {}

    for model_name, result in results.items():
        train_r2 = result['train_r2']
        test_r2 = result['test_r2']
        r2_gap = result['r2_gap']

        train_rmse = result['train_rmse']
        test_rmse = result['test_rmse']
        rmse_gap = result['rmse_gap']

        train_mape = result['train_mape']
        test_mape = result['test_mape']
        mape_gap = result['mape_gap']

        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–µ–ø–µ–Ω–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        if r2_gap > 0.15:
            overfitting_level = "–°–ò–õ–¨–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï"
            recommendation = "–£–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é, —É–º–µ–Ω—å—à–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"
        elif r2_gap > 0.08:
            overfitting_level = "–£–ú–ï–†–ï–ù–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï"
            recommendation = "–î–æ–±–∞–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é"
        elif r2_gap > 0.03:
            overfitting_level = "–°–õ–ê–ë–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï"
            recommendation = "–ü—Ä–∏–µ–º–ª–µ–º—ã–π —É—Ä–æ–≤–µ–Ω—å, –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É"
        else:
            overfitting_level = "–ú–ò–ù–ò–ú–ê–õ–¨–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï"
            recommendation = "–û—Ç–ª–∏—á–Ω–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ, –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞"

        overfitting_analysis[model_name] = {
            'overfitting_level': overfitting_level,
            'recommendation': recommendation,
            'r2_gap': r2_gap,
            'rmse_gap': rmse_gap,
            'mape_gap': mape_gap,
            'train_r2': train_r2,
            'test_r2': test_r2
        }

        print(f"\nüìä {model_name}:")
        print(f"   –£—Ä–æ–≤–µ–Ω—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {overfitting_level}")
        print(f"   R¬≤: Train={train_r2:.4f}, Test={test_r2:.4f}, Gap={r2_gap:.4f}")
        print(f"   RMSE Gap: {rmse_gap:,.0f} —Ä—É–±")
        print(f"   MAPE Gap: {mape_gap:.2f}%")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {recommendation}")

    return overfitting_analysis


# –ü–û–õ–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ï–ô –° –ú–ï–¢–†–ò–ö–ê–ú–ò –ü–û –î–ò–ê–ü–ê–ó–û–ù–ê–ú
def evaluate_model_detailed(model, X_train, X_test, y_train, y_test, model_name, categorical_features=None):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º"""
    try:
        # –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train_processed = X_train.copy()
        X_test_processed = X_test.copy()

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        numeric_cols = X_train_processed.select_dtypes(include=[np.number]).columns
        X_train_processed[numeric_cols] = X_train_processed[numeric_cols].fillna(
            X_train_processed[numeric_cols].median())
        X_test_processed[numeric_cols] = X_test_processed[numeric_cols].fillna(X_train_processed[numeric_cols].median())

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        if model_name in ['LightGBM', 'CatBoost'] and categorical_features:
            if model_name == 'LightGBM':
                # –î–ª—è LightGBM –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ category
                for col in categorical_features:
                    if col in X_train_processed.columns:
                        X_train_processed[col] = X_train_processed[col].astype('category')
                        X_test_processed[col] = X_test_processed[col].astype('category')
                model.fit(X_train_processed, y_train, categorical_feature=categorical_features)
            else:  # CatBoost
                cat_indices = [i for i, col in enumerate(X_train_processed.columns)
                               if col in categorical_features]
                model.fit(X_train_processed, y_train, cat_features=cat_indices, verbose=False)
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º Label Encoding
            from sklearn.preprocessing import LabelEncoder
            for col in categorical_features:
                if col in X_train_processed.columns:
                    le = LabelEncoder()
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ test –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                    combined = pd.concat([X_train_processed[col], X_test_processed[col]])
                    le.fit(combined)
                    X_train_processed[col] = le.transform(X_train_processed[col])
                    X_test_processed[col] = le.transform(X_test_processed[col])

            X_train_processed = X_train_processed.fillna(0)
            X_test_processed = X_test_processed.fillna(0)
            model.fit(X_train_processed, y_train)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_train = model.predict(X_train_processed)
        y_pred_test = model.predict(X_test_processed)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä—É–±–ª–∏
        y_train_rub = np.expm1(y_train)
        y_test_rub = np.expm1(y_test)
        y_pred_train_rub = np.expm1(y_pred_train)
        y_pred_test_rub = np.expm1(y_pred_test)

        # –í–°–ï –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ô –í–´–ë–û–†–ö–ï
        train_mse_rub = mean_squared_error(y_train_rub, y_pred_train_rub)
        train_mae_rub = mean_absolute_error(y_train_rub, y_pred_train_rub)
        train_rmse_rub = np.sqrt(train_mse_rub)
        train_r2_rub = r2_score(y_train_rub, y_pred_train_rub)
        train_mape = np.mean(np.abs((y_train_rub - y_pred_train_rub) / np.maximum(y_train_rub, 1))) * 100

        # –í–°–ï –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï
        test_mse_rub = mean_squared_error(y_test_rub, y_pred_test_rub)
        test_mae_rub = mean_absolute_error(y_test_rub, y_pred_test_rub)
        test_rmse_rub = np.sqrt(test_mse_rub)
        test_r2_rub = r2_score(y_test_rub, y_pred_test_rub)
        test_mape = np.mean(np.abs((y_test_rub - y_pred_test_rub) / np.maximum(y_test_rub, 1))) * 100

        # –ú–ï–¢–†–ò–ö–ò –í –õ–û–ì–ê–†–ò–§–ú–ò–ß–ï–°–ö–û–ô –®–ö–ê–õ–ï
        test_mse_log = mean_squared_error(y_test, y_pred_test)
        test_mae_log = mean_absolute_error(y_test, y_pred_test)

        # –†–ê–ó–ù–ò–¶–ê –ú–ï–ñ–î–£ TRAIN –ò TEST (–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)
        r2_gap = train_r2_rub - test_r2_rub
        mae_gap = train_mae_rub - test_mae_rub
        rmse_gap = train_rmse_rub - test_rmse_rub
        mape_gap = train_mape - test_mape

        # R¬≤ –ü–û –¶–ï–ù–û–í–´–ú –î–ò–ê–ü–ê–ó–û–ù–ê–ú
        price_range_metrics = calculate_price_range_r2(y_test_rub, y_pred_test_rub)

        return {
            'model': model,
            # –¢–†–ï–ù–ò–†–û–í–û–ß–ù–´–ï –ú–ï–¢–†–ò–ö–ò
            'train_r2': train_r2_rub,
            'train_mae': train_mae_rub,
            'train_rmse': train_rmse_rub,
            'train_mape': train_mape,
            'train_mse': train_mse_rub,
            # –¢–ï–°–¢–û–í–´–ï –ú–ï–¢–†–ò–ö–ò
            'test_r2': test_r2_rub,
            'test_mae': test_mae_rub,
            'test_rmse': test_rmse_rub,
            'test_mape': test_mape,
            'test_mse': test_mse_rub,
            # –ú–ï–¢–†–ò–ö–ò –í –õ–û–ì–ê–†–ò–§–ú–ò–ß–ï–°–ö–û–ô –®–ö–ê–õ–ï
            'mse_log': test_mse_log,
            'mae_log': test_mae_log,
            # –†–ê–ó–ù–ò–¶–ê (GAP)
            'r2_gap': r2_gap,
            'mae_gap': mae_gap,
            'rmse_gap': rmse_gap,
            'mape_gap': mape_gap,
            # –ú–ï–¢–†–ò–ö–ò –ü–û –î–ò–ê–ü–ê–ó–û–ù–ê–ú
            'price_range_r2': price_range_metrics['range_r2'],
            'weighted_range_r2': price_range_metrics['weighted_r2'],
            # –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê
            'predictions': y_pred_test_rub,
            'y_train_rub': y_train_rub,
            'y_test_rub': y_test_rub,
            'y_pred_train_rub': y_pred_train_rub,
            'y_pred_test_rub': y_pred_test_rub
        }

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ {model_name}: {e}")
        import traceback
        traceback.print_exc()
        return None


# –ë–´–°–¢–†–´–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ë–û–õ–¨–®–ò–• –î–ê–ù–ù–´–•
print("\n" + "=" * 80)
print("–ë–´–°–¢–†–´–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ë–û–õ–¨–®–ò–• –î–ê–ù–ù–´–•")
print("=" * 80)

models = {}

# 1. HistGradientBoosting - —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è
models['HistGradientBoosting'] = HistGradientBoostingRegressor(
    max_iter=200,
    learning_rate=0.1,
    max_depth=10,
    random_state=42,
    verbose=0
)

# 2. LightGBM - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è
if LIGHTGBM_AVAILABLE:
    models['LightGBM'] = lgb.LGBMRegressor(
        n_estimators=200,
        learning_rate=0.1,
        max_depth=10,
        num_leaves=63,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )

# 3. CatBoost - —Ö–æ—Ä–æ—à –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
if CATBOOST_AVAILABLE:
    models['CatBoost'] = CatBoostRegressor(
        iterations=200,
        learning_rate=0.1,
        depth=8,
        random_state=42,
        verbose=False,
        thread_count=-1
    )

# 4. Random Forest —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
models['RandomForest'] = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=20,
    min_samples_leaf=10,
    max_features=0.5,
    random_state=42,
    n_jobs=-1,
    verbose=0
)

# –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
cat_features = get_categorical_features(X)

# –î–ï–¢–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
print("\n–î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
results = {}
best_model = None
best_score = float('inf')

for name, model in models.items():
    print(f"\n–û–±—É—á–µ–Ω–∏–µ {name}...")
    start_time = pd.Timestamp.now()

    result = evaluate_model_detailed(model, X_train, X_test, y_train, y_test, name, cat_features)

    if result is not None:
        results[name] = result
        training_time = (pd.Timestamp.now() - start_time).total_seconds()

        print(f"‚úì {name} –æ–±—É—á–µ–Ω–∞ –∑–∞ {training_time:.1f} —Å–µ–∫")
        print(f"  R¬≤: Train = {result['train_r2']:.4f}, Test = {result['test_r2']:.4f}, Gap = {result['r2_gap']:.4f}")
        print(f"  RMSE: {result['test_rmse']:,.0f} —Ä—É–±, MAE: {result['test_mae']:,.0f} —Ä—É–±")
        print(f"  MAPE: {result['test_mape']:.2f}%")
        print(f"  –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π R¬≤ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º: {result['weighted_range_r2']:.4f}")

        if result['test_rmse'] < best_score:
            best_score = result['test_rmse']
            best_model = name
    else:
        print(f"‚úó {name} –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å")

if not results:
    raise ValueError("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")

print(f"\nüéØ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model} —Å RMSE {best_score:,.0f} —Ä—É–±–ª–µ–π")


# –ë–´–°–¢–†–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í –î–õ–Ø –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò
def fast_hyperparameter_tuning(best_model_name, X_train, y_train, X_test, y_test, categorical_features):
    """–ë—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    print(f"\n‚ö° –ë—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {best_model_name}...")

    if best_model_name == 'LightGBM' and LIGHTGBM_AVAILABLE:
        param_dist = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.05, 0.1, 0.15],
            'num_leaves': [31, 63, 127],
            'max_depth': [8, 10, 12]
        }

        model = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)

        random_search = RandomizedSearchCV(
            model, param_dist, n_iter=10, cv=2,
            scoring='neg_mean_squared_error', n_jobs=-1,
            random_state=42, verbose=1
        )

        X_processed = X_train.copy()
        if categorical_features:
            for col in categorical_features:
                if col in X_processed.columns:
                    X_processed[col] = X_processed[col].astype('category')

        random_search.fit(X_processed, y_train)
        return random_search.best_estimator_, random_search.best_params_

    elif best_model_name == 'CatBoost' and CATBOOST_AVAILABLE:
        param_dist = {
            'iterations': [200, 300, 400],
            'learning_rate': [0.05, 0.1, 0.15],
            'depth': [6, 8, 10],
            'l2_leaf_reg': [1, 3, 5]
        }

        model = CatBoostRegressor(random_state=42, verbose=False, thread_count=-1)

        random_search = RandomizedSearchCV(
            model, param_dist, n_iter=8, cv=2,
            scoring='neg_mean_squared_error', n_jobs=-1,
            random_state=42, verbose=1
        )

        random_search.fit(X_train, y_train)
        return random_search.best_estimator_, random_search.best_params_

    elif best_model_name == 'HistGradientBoosting':
        param_dist = {
            'max_iter': [200, 300, 400],
            'learning_rate': [0.05, 0.1, 0.15],
            'max_depth': [8, 10, 12],
            'min_samples_leaf': [10, 20, 30]
        }

        model = HistGradientBoostingRegressor(random_state=42, verbose=0)

        random_search = RandomizedSearchCV(
            model, param_dist, n_iter=8, cv=2,
            scoring='neg_mean_squared_error', n_jobs=-1,
            random_state=42, verbose=1
        )

        random_search.fit(X_train, y_train)
        return random_search.best_estimator_, random_search.best_params_

    else:  # RandomForest
        param_dist = {
            'n_estimators': [100, 150, 200],
            'max_depth': [10, 15, 20],
            'min_samples_split': [15, 20, 25],
            'max_features': [0.4, 0.5, 0.6]
        }

        model = RandomForestRegressor(random_state=42, n_jobs=-1, verbose=0)

        random_search = RandomizedSearchCV(
            model, param_dist, n_iter=8, cv=2,
            scoring='neg_mean_squared_error', n_jobs=-1,
            random_state=42, verbose=1
        )

        random_search.fit(X_train, y_train)
        return random_search.best_estimator_, random_search.best_params_


# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
best_params = None
try:
    tuned_model, best_params = fast_hyperparameter_tuning(
        best_model, X_train, y_train, X_test, y_test, cat_features
    )

    print(f"üéØ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {best_model}:")
    for param, value in best_params.items():
        print(f"  {param}: {value}")

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print(f"\n–û—Ü–µ–Ω–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ {best_model}...")
    tuned_result = evaluate_model_detailed(
        tuned_model, X_train, X_test, y_train, y_test, best_model, cat_features
    )

    if tuned_result:
        results[f'{best_model} (tuned)'] = tuned_result
        tuned_result['best_params'] = best_params  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–ª—É—á—à–∏–ª–∞ RMSE: {best_score:,.0f} -> {tuned_result['test_rmse']:,.0f} —Ä—É–±–ª–µ–π")

except Exception as e:
    print(f"‚ö†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
    print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")


# –°–û–•–†–ê–ù–ï–ù–ò–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í –í –§–ê–ô–õ
def save_hyperparameters(best_params, model_name, file_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
    if best_params:
        hyperparameters_data = {
            'model_name': model_name,
            'hyperparameters': best_params,
            'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(hyperparameters_data, f, indent=2, ensure_ascii=False)

        print(f"üíæ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {file_path}")
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")


# –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø
overfitting_analysis = analyze_overfitting(results)


# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
def plot_detailed_results(results):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))

    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    metrics_data = {}
    for model_name, result in results.items():
        metrics_data[model_name] = {
            'RMSE': result['test_rmse'],
            'MAE': result['test_mae'],
            'R¬≤': result['test_r2']
        }

    metrics_df = pd.DataFrame(metrics_data).T

    # RMSE –∏ MAE
    metrics_df[['RMSE', 'MAE']].plot(kind='bar', ax=axes[0, 0])
    axes[0, 0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ RMSE –∏ MAE (—Ä—É–±–ª–∏)')
    axes[0, 0].tick_params(axis='x', rotation=45)

    # R¬≤
    metrics_df[['R¬≤']].plot(kind='bar', ax=axes[0, 1])
    axes[0, 1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ R¬≤')
    axes[0, 1].tick_params(axis='x', rotation=45)

    # 2. R¬≤ –ø–æ —Ü–µ–Ω–æ–≤—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_name = min(results.items(), key=lambda x: x[1]['test_rmse'])[0]
    best_result = results[best_model_name]

    range_data = []
    for range_name, range_metrics in best_result['price_range_r2'].items():
        if not np.isnan(range_metrics['r2']):
            range_data.append({
                '–î–∏–∞–ø–∞–∑–æ–Ω': range_name,
                'R¬≤': range_metrics['r2'],
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤': range_metrics['samples']
            })

    if range_data:
        range_df = pd.DataFrame(range_data)
        axes[0, 2].bar(range_df['–î–∏–∞–ø–∞–∑–æ–Ω'], range_df['R¬≤'])
        axes[0, 2].set_title(f'R¬≤ –ø–æ —Ü–µ–Ω–æ–≤—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º ({best_model_name})')
        axes[0, 2].tick_params(axis='x', rotation=45)

        for i, v in enumerate(range_df['R¬≤']):
            axes[0, 2].text(i, v + 0.01, f"n={range_df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤'].iloc[i]}",
                            ha='center', va='bottom', fontsize=8)

    # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    best_predictions = best_result['predictions']
    y_test_rub = best_result['y_test_rub']

    axes[1, 0].scatter(y_test_rub, best_predictions, alpha=0.5)
    axes[1, 0].plot([y_test_rub.min(), y_test_rub.max()],
                    [y_test_rub.min(), y_test_rub.max()], 'r--', lw=2)
    axes[1, 0].set_xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–∞')
    axes[1, 0].set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞')
    axes[1, 0].set_title(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è ({best_model_name})')

    # 4. –û—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    errors = best_predictions - y_test_rub
    axes[1, 1].hist(errors, bins=50, alpha=0.7)
    axes[1, 1].axvline(x=0, color='r', linestyle='--')
    axes[1, 1].set_xlabel('–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (—Ä—É–±–ª–∏)')
    axes[1, 1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[1, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')

    # 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Train/Test R¬≤
    model_names = list(results.keys())
    train_r2 = [results[name]['train_r2'] for name in model_names]
    test_r2 = [results[name]['test_r2'] for name in model_names]

    x = np.arange(len(model_names))
    width = 0.35

    axes[1, 2].bar(x - width / 2, train_r2, width, label='Train R¬≤', alpha=0.7)
    axes[1, 2].bar(x + width / 2, test_r2, width, label='Test R¬≤', alpha=0.7)
    axes[1, 2].set_xlabel('–ú–æ–¥–µ–ª–∏')
    axes[1, 2].set_ylabel('R¬≤')
    axes[1, 2].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ R¬≤ –Ω–∞ Train –∏ Test –≤—ã–±–æ—Ä–∫–∞—Ö')
    axes[1, 2].set_xticks(x)
    axes[1, 2].set_xticklabels(model_names, rotation=45)
    axes[1, 2].legend()

    plt.tight_layout()
    plt.show()


# –°—Ç—Ä–æ–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
plot_detailed_results(results)

# –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ò –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í
final_model_name = best_model
final_model = results[final_model_name]['model']
final_best_params = None

if f'{best_model} (tuned)' in results:
    final_model_name = f'{best_model} (tuned)'
    final_model = results[final_model_name]['model']
    final_best_params = results[final_model_name].get('best_params')

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
model_path = '/Users/mariabug/ML/data/detailed_car_price_predictor.pkl'
joblib.dump(final_model, model_path)
print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ '{model_path}'")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
if final_best_params:
    hyperparams_path = '/Users/mariabug/ML/data/best_hyperparameters.json'
    save_hyperparameters(final_best_params, final_model_name, hyperparams_path)

# –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\n" + "=" * 80)
print("–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("=" * 80)

# –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –º–æ–¥–µ–ª—è–º
summary_data = []
for model_name, result in results.items():
    summary_data.append({
        'Model': model_name,
        'Train R¬≤': f"{result['train_r2']:.4f}",
        'Test R¬≤': f"{result['test_r2']:.4f}",
        'R¬≤ Gap': f"{result['r2_gap']:.4f}",
        'Test RMSE': f"{result['test_rmse']:,.0f}",
        'Test MAE': f"{result['test_mae']:,.0f}",
        'Test MAPE': f"{result['test_mape']:.2f}%",
        'Weighted R¬≤': f"{result['weighted_range_r2']:.4f}"
    })

summary_df = pd.DataFrame(summary_data)
print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
print(summary_df.to_string(index=False))

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
print(f"\nüéØ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {final_model_name}")
best_result = results[final_model_name]

print(f"\nüìà –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
print(f"   R¬≤: Train = {best_result['train_r2']:.4f}, Test = {best_result['test_r2']:.4f}")
print(f"   RMSE: {best_result['test_rmse']:,.0f} —Ä—É–±")
print(f"   MAE: {best_result['test_mae']:,.0f} —Ä—É–±")
print(f"   MAPE: {best_result['test_mape']:.2f}%")
print(f"   –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π R¬≤ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º: {best_result['weighted_range_r2']:.4f}")

# –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
if final_model_name in overfitting_analysis:
    analysis = overfitting_analysis[final_model_name]
    print(f"\n‚ö†Ô∏è  –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø:")
    print(f"   –£—Ä–æ–≤–µ–Ω—å: {analysis['overfitting_level']}")
    print(f"   R¬≤ Gap: {analysis['r2_gap']:.4f}")
    print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {analysis['recommendation']}")

# R¬≤ –ø–æ —Ü–µ–Ω–æ–≤—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
print(f"\nüéØ R¬≤ –ü–û –¶–ï–ù–û–í–´–ú –î–ò–ê–ü–ê–ó–û–ù–ê–ú:")
print("-" * 70)
for range_name, range_metrics in best_result['price_range_r2'].items():
    r2_value = range_metrics['r2']
    samples = range_metrics['samples']
    weight = range_metrics['weight'] * 100

    if not np.isnan(r2_value):
        print(f"   {range_name:25} | R¬≤ = {r2_value:7.4f} | –û–±—Ä–∞–∑—Ü–æ–≤ = {samples:4d} | –î–æ–ª—è = {weight:5.1f}%")
    else:
        print(f"   {range_name:25} | R¬≤ = {'N/A':7} | –û–±—Ä–∞–∑—Ü–æ–≤ = {samples:4d} | –î–æ–ª—è = {weight:5.1f}%")

# –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
print(f"\nüìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•:")
print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

print(f"\n‚ö° –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
if final_best_params:
    print(f"üìÅ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {hyperparams_path}")
